batch_size: 512
learning_rate: 0.005
epochs: 50
mlp_emb_size: 64
gmf_emb_size: 32
num_negatives: 4
layers: [64,32,16,8]
l2_reg: 0.0000001

save_best: True