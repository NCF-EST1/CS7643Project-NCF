batch_size: 256
learning_rate: 0.001
epochs: 3
mlp_emb_size: 8
gmf_emb_size: 8
num_negatives: 4
layers: [16,64,32,16,8]
l2_reg: 0.0000001

save_best: False